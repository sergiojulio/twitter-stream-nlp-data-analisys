#version: "3.1"
version: "3.8"
services:
  app:
    container_name: python-server
    build: 
      context: ./ # specify context, this case root pah
      dockerfile: ./docker/app/Dockerfile # location
    # command: uvicorn src.app.fastapi.main:app --host 0.0.0.0 --port 8080 --reload
    command: uvicorn src.app.fastapi.main:app --host 0.0.0.0 --port 8080
    ports:
      - 8080:8080 # fastapi
      - 8501:8501 # streamlit
    volumes:
      - .:/code # add in preferences resources (workin directory)
    environment:
      - KAFKA_SERVER=kafka:9093
      - KAFKA_TOPIC=kafka_topic
      - MASTODON_KEY_WORD_LIST=usa,ukraine,russia,nato,palestine,israel,iran
      - POSTGRES_SERVER=postgres:5432
      - POSTGRES_DB=postgres_db
      - POSTGRES_USER=postgres
      - POSTGRES_PASS=postgres      
    # env_file:
    #   - ../.env.development
    # depends_on:
    #   kafka:
    #     condition: service_healthy

  zookeeper:
    container_name: zookeeper-server
    image: docker.io/bitnami/zookeeper:3.8
    ports:
      - "2181:2181"
    volumes:
      - "zookeeper_data:/bitnami"
    environment:
      - ALLOW_ANONYMOUS_LOGIN=yes
    # zookeeper.connection.timeout.ms=60000

  kafka:
    container_name: kafka-server
    image: docker.io/bitnami/kafka:3.4
    ports:
      - "9092:9092"
    volumes:
      - "kafka_data:/bitnami"
    environment:
      - BITNAMI_DEBUG=yes
      - KAFKA_CFG_ZOOKEEPER_CONNECT=zookeeper:2181
      - ALLOW_PLAINTEXT_LISTENER=yes
      - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=CLIENT:PLAINTEXT,EXTERNAL:PLAINTEXT
      - KAFKA_CFG_LISTENERS=CLIENT://:9093,EXTERNAL://:9092
      - KAFKA_CFG_ADVERTISED_LISTENERS=CLIENT://kafka:9093,EXTERNAL://kafka:9092
      - KAFKA_CFG_INTER_BROKER_LISTENER_NAME=CLIENT
    depends_on:
      - zookeeper
    # healthcheck:
    #   test: ["CMD", "/opt/bitnami/kafka/bin/kafka-topics.sh", "--list", "--zookeeper", "zookeeper:2181"]
    #   interval: 5s
    #   timeout: 10s
    #   retries: 5

  spark:
    container_name: spark-server
    build: 
      context: ./ # specify context, this case root pah
      dockerfile: ./docker/spark/Dockerfile # location
    #command: bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
    #entrypoint: docker/development-entrypoint.sh
    environment:
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - SPARK_USER=spark
      - KAFKA_TOPIC=kafka_topic
      - KAFKA_SERVER=kafka:9093
      - POSTGRES_SERVER=postgres:5432
      - POSTGRES_DB=postgres_db
      - POSTGRES_USER=postgres
      - POSTGRES_PASS=postgres

    ports:
      - '4040:4040' # UI
    volumes:
      - .:/code # add in preferences resources (workin directory)
    depends_on:
      - postgres      

  spark-worker:
    container_name: worker-server
    # install python modules command: ?
    build: 
      context: ./ # specify context, this case root pah
      dockerfile: ./docker/spark/Dockerfile # location
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark:7077
      - SPARK_WORKER_MEMORY=3G
      - SPARK_WORKER_CORES=4
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - SPARK_USER=spark
      - KAFKA_TOPIC=kafka_topic
      - KAFKA_SERVER=kafka:9093
      - POSTGRES_SERVER=postgres:5432
      - POSTGRES_DB=postgres_db
      - POSTGRES_USER=postgres
      - POSTGRES_PASS=postgres


    volumes:
      - .:/code # add in preferences resources (work directory)

  postgres:
    container_name: postgres-server
    image: postgres:14.1-alpine
    # restart: always
    environment:
      - POSTGRES_DB=postgres_db
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=postgres
      - POSTGRES_ROOT_PASSWORD=root
    ports:
      - '5432:5432'
    volumes:
      - ./postgres/data:/var/lib/postgresql/data
      - ./postgres/init.sql:/docker-entrypoint-initdb.d/init.sql

volumes:
  app:
    driver: local
  zookeeper_data:
    driver: local
  kafka_data:
    driver: local
  postgres-data:  
  spark:
    driver: local